{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHjRPi1Is92G"
      },
      "source": [
        "Вопросы:\n",
        " - почему результат меняется при каждом запуске, хотя стоит seed=0 при инициализации весов? Что нужно ещё добавить?\n",
        " - объяснить рез-тат для модели на искаженных данных (val_acc vs. train_acc)\n",
        " - augmentation не помогает улучшить, а только ухудшает рез-тат. Почему?\n",
        " - классический пример обьяснения, почему работают нейросети, в частности CNN: первый слой из пикселей собирает edges, из них собираются отдельные части (нос, глаза, и тд), и так далее...можно ли как-то следуя этой интуиции прикинуть кол-во слоёв и кол-во элементов в сетке?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqHfgukBs92H"
      },
      "source": [
        "# Convolutional Neural Networks: Application\n",
        "\n",
        "Here we will try a CNN model on the SIGNS dataset. The SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.\n",
        "\n",
        "<img src=\"https://github.com/sersavsnz/exercise.hand_signs_recognition/blob/main/images/SIGNS.png?raw=1\" style=\"width:800px;height:300px;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5mgDYO5s92I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from cnn_utils import *\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import datetime\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unVKpUZ6s92J"
      },
      "outputs": [],
      "source": [
        "# # set reproducible results ???\n",
        "# from numpy.random import seed\n",
        "# seed(1)\n",
        "# tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njQR3Qyks92J"
      },
      "outputs": [],
      "source": [
        "# Loading the data (signs)\n",
        "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJRLdxJls92J"
      },
      "source": [
        "## Prepare the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhR2It3ns92J"
      },
      "outputs": [],
      "source": [
        "X_train_orig.shape, X_test_orig.shape, Y_train_orig.shape, Y_test_orig.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWTY6iB5s92J"
      },
      "outputs": [],
      "source": [
        "x_train, x_test = X_train_orig / 255., X_test_orig / 255.\n",
        "y_train, y_test = np.squeeze(Y_train_orig), np.squeeze(Y_test_orig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6H3WqvMs92K"
      },
      "source": [
        "## Baseline model\n",
        "\n",
        "As a baseline we take the model from Coursera exercise.\n",
        "\n",
        "Result: train_acc = 0.95, test_acc = 0.85 (once had train_acc = 0.98, test_acc = 0.9) - **add regularization.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMKyZWCps92K"
      },
      "outputs": [],
      "source": [
        "def create_baseline_model():\n",
        "    \"\"\"\n",
        "    Specifies the architecture of our CNN. \n",
        "    return :: uncompiled model\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(64, 64, 3))\n",
        "\n",
        "    C1 = layers.Conv2D(8, 4, activation='relu', strides=(1, 1), padding='same', \n",
        "                       kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0))(inputs) \n",
        "    \n",
        "    P1 = layers.MaxPool2D(pool_size=(8, 8), strides=(8, 8), padding='same')(C1) \n",
        "    \n",
        "    C2 = layers.Conv2D(16, 2, activation='relu', strides=(1, 1), padding='same',\n",
        "                      kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0))(P1)\n",
        "    \n",
        "    P2 = layers.MaxPool2D(pool_size=(4, 4), strides=(4, 4), padding='same')(C2)\n",
        "\n",
        "    F = layers.Flatten()(P2)\n",
        "\n",
        "    outputs = layers.Dense(6, activation=\"softmax\")(F)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def compile_baseline_model(model, learning_rate):\n",
        "    \"\"\"\n",
        "    Specifies optimizer, loss function and evaluation metrics. \n",
        "    return :: compiled model\n",
        "    \"\"\"\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def get_summary(model):\n",
        "    \"\"\"\n",
        "    Prints model's summary\n",
        "    \"\"\"\n",
        "    model.summary()\n",
        "    \n",
        "    \n",
        "def evaluate_model(model):\n",
        "    \"\"\"\n",
        "    Evaluates model's performance\n",
        "    \"\"\"\n",
        "    \n",
        "    results = model.evaluate(x_train, y_train, batch_size=None)\n",
        "    print(\"train loss, train acc:\", results)\n",
        "\n",
        "    results = model.evaluate(x_test, y_test, batch_size=None)\n",
        "    print(\"test loss, test acc:\", results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsEKQc_ts92L"
      },
      "outputs": [],
      "source": [
        "baseline_model = create_baseline_model()\n",
        "baseline_model = compile_baseline_model(baseline_model, learning_rate=0.001)\n",
        "\n",
        "get_summary(baseline_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "DJqN8wvfs92L"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 300\n",
        "\n",
        "! rm -rf ./logs/fit/baseline/\n",
        "\n",
        "log_dir = \"./logs/fit/baseline/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "history = baseline_model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "    callbacks=[tensorboard_callback]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkup3WkMs92L"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit/baseline/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqIJJl8Bs92L"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model \n",
        "evaluate_model(baseline_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAqvljKws92L"
      },
      "source": [
        "## Expand the dataset - data augmentation\n",
        "\n",
        "Let us generate more data by flipping and 90 deg. rotating the original images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu4KNz4ks92M"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "x_aug = [cv2.flip(img, 1) for img in x_train]\n",
        "x_train_aug = np.append(x_train, x_aug, axis=0)\n",
        "y_train_aug = np.append(y_train, y_train, axis=0)\n",
        "\n",
        "x_aug = [cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE) for img in x_train]\n",
        "x_train_aug = np.append(x_train_aug, x_aug, axis=0)\n",
        "y_train_aug = np.append(y_train_aug, y_train, axis=0)\n",
        "\n",
        "x_aug = [cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE) for img in x_train]\n",
        "x_train_aug = np.append(x_train_aug, x_aug, axis=0)\n",
        "y_train_aug = np.append(y_train_aug, y_train, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB4yYFS2s92M"
      },
      "source": [
        "#### base-line model\n",
        "Check whether we can improve the accuracy by adding more data.\n",
        "\n",
        "Result: training is much slower, no improvement (probably should use **learning rate decay**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aGMyG_MMs92M"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 500\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "baseline_model = create_baseline_model()\n",
        "baseline_model = compile_baseline_model(baseline_model, learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Fit the model\n",
        "! rm -rf ./logs/fit/baseline_aug/\n",
        "\n",
        "log_dir = \"./logs/fit/baseline_aug/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "history = baseline_model.fit(\n",
        "    x_train_aug, y_train_aug,\n",
        "    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "    callbacks=[tensorboard_callback]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OuwyWPns92M"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit/baseline_aug/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tqljy8ds92M"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "evaluate_model(baseline_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p9Zn4E_s92N"
      },
      "source": [
        "## Final model\n",
        "\n",
        "We use L2-regularization and Droupout layers to reduce overfitting. Learning rate decay is added for better optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYQXFCV4s92N"
      },
      "outputs": [],
      "source": [
        "def create_model(regularizer):\n",
        "    \"\"\"\n",
        "    Specifies the architecture of our CNN. \n",
        "    return :: uncompiled model\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(64, 64, 3), name=\"signs\")\n",
        "\n",
        "    C1 = layers.Conv2D(32, 5, activation='relu', strides=(1, 1), padding='same', \n",
        "                      kernel_regularizer=regularizer)(inputs) \n",
        "    \n",
        "    P1 = layers.MaxPool2D(pool_size=(4, 4), strides=(4, 4), padding='same')(C1)\n",
        "    \n",
        "    C2 = layers.Conv2D(64, 3, activation='relu', strides=(1, 1), padding='same',\n",
        "                      kernel_regularizer=regularizer)(P1) \n",
        "    \n",
        "    P2 = layers.MaxPool2D(pool_size=(4, 4), strides=(4, 4), padding='same')(C2) \n",
        "\n",
        "#     C3 = layers.Conv2D(64, 3, activation='relu', strides=(1, 1), padding='same',\n",
        "#                       kernel_regularizer=regularizer)(P2) # CONV2D: stride 1, padding 'SAME'\n",
        "    \n",
        "    F = layers.Flatten()(P2)\n",
        "    \n",
        "    A1 = layers.Dense(120, activation=\"relu\", \n",
        "                           kernel_regularizer=regularizer)(F)\n",
        "    D1 = tf.keras.layers.Dropout(0.1)(A1)\n",
        "    \n",
        "    A2 = layers.Dense(84, activation=\"relu\", \n",
        "                           kernel_regularizer=regularizer)(D1)\n",
        "    D2 = tf.keras.layers.Dropout(0.1)(A2)\n",
        "\n",
        "    outputs = layers.Dense(6, activation=\"softmax\", name=\"predictions\", \n",
        "                           kernel_regularizer=regularizer)(D2)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def compile_model(model, learning_rate):\n",
        "    \"\"\"\n",
        "    Specifies optimizer, loss function and evaluation metrics. \n",
        "    return :: compiled model\n",
        "    \"\"\"\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTH4ou_Ms92N"
      },
      "source": [
        "#### original data\n",
        "\n",
        "Result: train_acc = 0.98, test_acc = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pqIsHdw9s92N"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-3\n",
        "REGULARIZER = tf.keras.regularizers.L2(0.005)\n",
        "BATCH_SIZE = 64\n",
        "# BATCH_SIZE = x_train.shape[0]\n",
        "EPOCHS = 100\n",
        "REDUCE_LR_FACTOR = 0.5\n",
        "\n",
        "model = create_model(regularizer=REGULARIZER)\n",
        "model = compile_model(model, learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Fit the model\n",
        "! rm -rf ./logs/fit/model\n",
        "log_dir = \"./logs/fit/model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "# learning rate reduce by factor\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=REDUCE_LR_FACTOR,\n",
        "                              patience=5, min_lr=1e-6) #monitor='val_loss'\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "    validation_split=0.3,\n",
        "    callbacks=[tensorboard_callback, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymb0XSlbs92O"
      },
      "outputs": [],
      "source": [
        "get_summary(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys3iAPlGs92O"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit/model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5RGasv1s92O"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "evaluate_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHQcFO5ts92O"
      },
      "source": [
        "#### augmented data\n",
        "\n",
        "Result: is quite funny for val_acc vs. train_acc and test_acc. \n",
        "Why so?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0_Uxe4Ps92O"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-3\n",
        "REGULARIZER = tf.keras.regularizers.L2(0.005)\n",
        "BATCH_SIZE = 64\n",
        "# BATCH_SIZE = x_train.shape[0]\n",
        "EPOCHS = 150\n",
        "REDUCE_LR_FACTOR = 0.5\n",
        "\n",
        "model = create_model(regularizer=REGULARIZER)\n",
        "model = compile_model(model, learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Fit the model\n",
        "! rm -rf ./logs/fit/model\n",
        "log_dir = \"./logs/fit/model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "# learning rate reduce by factor\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=REDUCE_LR_FACTOR,\n",
        "                              patience=5, min_lr=1e-6) #monitor='val_loss'\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_aug, y_train_aug,\n",
        "    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "    validation_split=0.3,\n",
        "    callbacks=[tensorboard_callback, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht5Oe4WQs92O"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit/model_aug/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKzo9jIvs92O"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "evaluate_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra_Bmqpfs92O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ef35y7as92O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffkRVuafs92P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK9VTo-ys92P"
      },
      "source": [
        "## Debugging stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MpdsyKLs92P"
      },
      "outputs": [],
      "source": [
        "# # Generate predictions\n",
        "# predictions = model.predict(x_test)\n",
        "\n",
        "# y_pred = np.argmax(predictions, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfTcndYxs92P"
      },
      "outputs": [],
      "source": [
        "# false_pred_index = [i for i,_ in enumerate(y_pred) if y_pred[i]!=y_test[i]]\n",
        "# true_pred_index = [i for i,_ in enumerate(y_pred) if y_pred[i]==y_test[i]]\n",
        "# false_pred_labels = [y_test[ind] for ind in false_pred_index]\n",
        "# true_pred_labels = [y_test[ind] for ind in true_pred_index]\n",
        "\n",
        "# print(f\"True predicted labels: {np.unique(true_pred_labels, return_counts=True)}\")\n",
        "# print(f\"False predicted labels: {np.unique(false_pred_labels, return_counts=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siNcusX2s92P"
      },
      "outputs": [],
      "source": [
        "# for index in false_pred_index:\n",
        "#     print (f\"i: {index}, pred: y = {y_pred[index]} \\t true: y = {y_test[index]}\")\n",
        "#     plt.imshow(X_test_orig[index])\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuih47Ips92P"
      },
      "outputs": [],
      "source": [
        "# for index in true_pred_index:\n",
        "#     print (\"pred: y = \" + str(y_pred[index]) + \"\\t\" + \"true: y = \" + str(y_test[index]))\n",
        "#     plt.imshow(X_test_orig[index])\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sXSRPhys92P"
      },
      "outputs": [],
      "source": [
        "# for i in false_pred_index:\n",
        "#     print(i, list(predictions[i]))\n",
        "#     print(y_pred[i], y_test[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvJiRiPOs92P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0keLryfNs92Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "convolutional-neural-networks",
      "graded_item_id": "bwbJV",
      "launcher_item_id": "0TkXB"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}